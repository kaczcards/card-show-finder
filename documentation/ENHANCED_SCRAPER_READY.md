# Enhanced Scraper â€“ Ready for Prime-Time âœ…

Welcome to the final hand-off guide for the **Card-Show-Finder enhanced scraper**.  
Everything you need to understand what was fixed, why it matters, and how to run the new pipeline is here.

---

## 1. Whatâ€™s Been Accomplished So Far

| Area | Status | Highlights |
|------|--------|------------|
| Broken deletion tooling | âœ… Fixed | `delete-first-n-records.js` + patched `delete-records.js` now accept UUIDs instead of integers. |
| Environment handling | âœ… Added | `scraper/setup-env.js` maps Expo & legacy variable names â†’ correct ones, validates presence. |
| Data quality pipeline | âœ… Added | Intelligent date / location / contact parsing, entry-fee detection, show-hours extraction, Google Maps geocoding. |
| AI-powered extraction | âœ… Added | Gemini-based prompts including a specialised Sports Collectors Digest (SCD) prompt. |
| Safety nets | âœ… Added | Validation with errors / warnings, optional `--dry-run`, verbose logging, inspection & repair of single records. |

---

## 2. The Old Data-Quality Problems (Real Examples)

Below are five records we **just deleted** from `scraped_shows_pending`.  
They illustrate every pain-point the new scraper solves:

| UUID (truncated) | Raw `startDate` | Raw `location` | Issues Seen |
|------------------|-----------------|----------------|-------------|
| 2945d9â€¦ | `January 5-6, 2025` | `The Venue, Montgomery, AL` | Past date (already 2025) + generic venue string |
| 2aee76â€¦ | `Aug 2 AL` | `Jaycees Community Building, John Hunt Park, Huntsville, AL` | **State code embedded in date** (â€œALâ€), needs cleanup |
| 65b1c7â€¦ | `Aug 9 AL` | `Gardendale Civic Center, Gardendale, AL` | Same date problem, no ZIP, no coordinates |
| 6bca92â€¦ | `March 15-16, 2025` | `The Venue, Tucson, AZ` | Past date, missing address split, no contact |
| f94a96â€¦ | `February 1-2, 2025` | `The Venue, Phoenix, AZ` | Same as above |

*Result:* front-end map couldnâ€™t place events, duplicates flourished, and users saw â€œAug 2 ALâ€ as a date.

---

## 3. What the Enhanced Scraper Does Now

1. **AI Extraction** â€“ Gemini 1.5 prompts tailored to generic pages *and* Sports Collectors Digest.
2. **Date Normalisation**  
   â€¢ Removes stray state codes â†’ â€œAug 2 ALâ€ â†’ â€œAug 2â€.  
   â€¢ Interprets ranges â†’ ISO `startDate` / `endDate`.  
   â€¢ Rolls forward to next year if scraped date is already in the past.
3. **Location Parsing** â€“ Splits venue / address / city / state / ZIP, even when jammed together.
4. **Contact Extraction** â€“ Pulls name / phone / email from one messy string.
5. **Entry Fee & Hours** detection with common patterns (â€œFreeâ€, â€œ$5â€, â€œ9 am â€“ 3 pmâ€).
6. **Geocoding** (Google Maps) adds `latitude` / `longitude` for pin-drops.
7. **Validation** â€“ Flags missing essentials, prevents saving obviously broken records.
8. **Tooling** â€“  
   â€¢ `--dry-run` mode (no DB writes)  
   â€¢ `--verbose` deep logging  
   â€¢ `--inspect-id <uuid>` to repair a single row  
   â€¢ Priority scoring RPC updates when saving.

---

## 4. Get Your Google AI (Gemini) API Key

The AI step is blocked until an API key is present.

1. Go to **https://makersuite.google.com** and sign in.  
2. Left sidebar â†’ **API keys** â†’ **Create API key**.  
3. Copy the key (looks like `AIzaSy...`).  
4. Open the project `.env` file (root of repo).  
5. Add / edit this line:

```
GOOGLE_AI_KEY=AIzaSyYourNewKeyHere
```

6. **Reload** your terminal environment:

```bash
# from project root
source .env
node scraper/setup-env.js   # should show âœ“ for GOOGLE_AI_KEY
```

---

## 5. Test the Enhanced Scraper on Sports Collectors Digest

```bash
# 1. Ensure env is loaded / mapped
node scraper/setup-env.js

# 2. Dry-run, verbose (no DB writes)
node scraper/enhanced-scraper.js \
     --url https://sportscollectorsdigest.com/show-calendar \
     --verbose --dry-run
```

What you should see:

* â€œHTML fetched â€¦ bytesâ€  
* AI chunk logs: `Chunk 1 â‡’ 40 shows` (numbers will vary)  
* â€œNormalized 38 valid shows (0 invalid, 2 with warnings)â€  
* Geocoding progress (if `--geocode` not disabled)  
* Final summary with `showsFound` > 0.

If you drop `--dry-run` the events will be saved to `scraped_shows_pending` with full `normalized_json` & optional `geocoded_json`.

---

## 6. Example of Improved Output

A single show after normalisation (excerpt):

```json
{
  "name": "Huntsville Sports Cards & Collectibles Show",
  "startDate": "2025-08-02T05:00:00.000Z",
  "endDate": "2025-08-02T05:00:00.000Z",
  "venueName": "Jaycees Community Building",
  "address": "2180 Airport Rd SW",
  "city": "Huntsville",
  "state": "AL",
  "zipCode": "35802",
  "coordinates": {
    "latitude": 34.6889,
    "longitude": -86.5861,
    "formattedAddress": "2180 Airport Rd SW, Huntsville, AL 35802, USA"
  },
  "entryFee": "Free admission",
  "showHours": "9am - 3pm",
  "contactName": "John Smith",
  "contactPhone": "256-555-1234",
  "contactEmail": "john@collectibleshow.com"
}
```

Contrast that with the deleted recordâ€™s raw payload:

```
Date: "Aug 2 AL"
Location: "Jaycees Community Building, John Hunt Park, Huntsville, AL"
No coordinates, no contact, mixed date/state.
```

---

## 7. Clear Next Steps

1. **Add GOOGLE_AI_KEY** to `.env` (section 4).  
2. Run the **dry run** command above â€“ confirm valid shows appear.  
3. Remove `--dry-run` and run again â†’ events populate the database.  
4. Visit your front-end map â€“ pins should display with accurate coordinates.  
5. Spot-check a few records in Supabase â†’ `normalized_json` looks like the example.  
6. Use `delete-first-n-records.js` or `--inspect-id` to clean / adjust as needed.  
7. (Optional) Run the scraper for a single state:

```bash
node scraper/enhanced-scraper.js --state TX
```

8. Schedule the scraper (GitHub Action / Supabase Edge Function) once manual tests pass.

Happy collecting â€“ the scraper is now **enhanced and ready**! ğŸ‰
